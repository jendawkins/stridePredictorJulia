{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "\n",
    "import matlab.engine\n",
    "import scipy.signal\n",
    "eng = matlab.engine.start_matlab()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# various helper functions\n",
    "def one_hot(data, alphabet):\n",
    "    return (np.arange(len(alphabet)+1) == data[...,None]).astype(int)\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader/processor function\n",
    "def readLangs(filter):\n",
    "    print(\"Reading data...\")\n",
    "\n",
    "    Fs = 1/(10/1000)\n",
    "    Wn1 = 5/(Fs/2)\n",
    "    Wn2 = .8/(Fs/2)\n",
    "\n",
    "    bl, al = scipy.signal.butter(N=4, Wn=Wn1, btype='low')\n",
    "    bh, ah = scipy.signal.butter(N=4, Wn=Wn2, btype='high')\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    start_time = 0;\n",
    "    cycle_mat = [];\n",
    "    data_fin = np.empty((0,7))\n",
    "    strides_bool_fin = np.empty(0)\n",
    "    for f in glob.glob(\"*.csv\"):\n",
    "        print(f)\n",
    "        x = np.genfromtxt(f,delimiter=',', skip_header=1)[1:,[1,3,4,5,6,7,8]]\n",
    "        x[:,1:] = x[:,1:] - np.mean(x[:,1:], axis = 0)\n",
    "        fin_time = np.arange(start_time, x[-1,0]+start_time, x[-1,0]/x.shape[0])\n",
    "        fin_time = fin_time[:x.shape[0]]\n",
    "        x = x[:len(fin_time),:]\n",
    "        # if len(fin_time) != x.shape[0]:\n",
    "        x_0 = np.empty(x.shape)\n",
    "        x_int = np.empty(x.shape)\n",
    "        for i in range(1,x.shape[1]):\n",
    "            x_int[:,i] = np.interp(fin_time, x[:,0]+start_time, x[:,i])\n",
    "            # low pass filtering\n",
    "            x_0[:,i] = x_int[:,i];\n",
    "            if filter:\n",
    "                x_int[:,i] = scipy.signal.filtfilt(bl, al, x_int[:,i])\n",
    "            # high pass filtering\n",
    "                x_int[:,i] = scipy.signal.filtfilt(bh, ah, x_int[:,i])\n",
    "\n",
    "        avg_int = np.mean(fin_time[2:-1]-fin_time[1:-2])\n",
    "        sig = np.sqrt(np.sum(np.square(np.divide(x_int[:,1:4], x_int[:,1:4].max(axis=0))),axis=1))\n",
    "        if f[0] == 'S':\n",
    "            pk_perc = 50\n",
    "            pk_dist = 130\n",
    "        else:\n",
    "            pk_perc = 90\n",
    "            pk_dist = 90\n",
    "        x_int[:,0] = fin_time\n",
    "\n",
    "        sig_in = matlab.double(list(sig))\n",
    "        sig_in2 = matlab.double(list(sig[1:750]))\n",
    "        strides, locs = eng.findpeaks(sig_in,\"MinPeakHeight\",float(np.percentile(sig,pk_perc)),\"MinPeakDistance\",pk_dist,nargout=2)\n",
    "        strides2, locs2 = eng.findpeaks(sig_in2,\"MinPeakHeight\",float(np.percentile(sig[1:750],97)),nargout=2)\n",
    "        strides = np.array(strides)\n",
    "        locs = np.array(locs).astype(int)\n",
    "        locs2 = np.array(locs2)\n",
    "        strides = strides[locs>1000]\n",
    "        locs = locs[locs>1000]\n",
    "\n",
    "        # fig, axes = plt.subplots(2, 1)\n",
    "        # axes[0].plot(fin_time, x_0[:,3])\n",
    "        # axes[1].plot(fin_time, x_int[:,3])\n",
    "        # axes[1].scatter(fin_time[locs], strides, c=\"g\")\n",
    "        # axes[0].scatter(fin_time[locs], strides, c=\"g\")\n",
    "\n",
    "        # import pdb; pdb.set_trace()\n",
    "        # plt.plot(fin_time,np.squeeze(sig_in))\n",
    "        # plt.scatter(fin_time[locs],strides)\n",
    "\n",
    "        strides_bool = np.zeros(len(fin_time))\n",
    "        strides_bool[locs] = np.ones(len(locs))\n",
    "\n",
    "        calib_pt = round(np.mean(locs2))\n",
    "        keep_inds = np.zeros(1,locs[0])\n",
    "        for l, sloc in enumerate(locs[1:]):\n",
    "            s_to_s = abs(sloc-locs[l])+1\n",
    "            if s_to_s*avg_int > 2200:\n",
    "                keep_inds = np.append(keep_inds, np.zeros(s_to_s+1))\n",
    "            else:\n",
    "                keep_inds = np.append(keep_inds, np.ones(s_to_s+1))\n",
    "                cycle_mat = np.append(cycle_mat, np.arange(s_to_s+1)/s_to_s)\n",
    "        keep_inds = np.append(keep_inds, np.zeros(len(fin_time)-len(keep_inds)))\n",
    "        keep_inds = np.where(keep_inds)[0]\n",
    "        data = x_int[keep_inds,:]\n",
    "        data_fin = np.vstack((data_fin, data))\n",
    "        strides_bool = strides_bool[keep_inds]\n",
    "        strides_bool_fin = np.append(strides_bool_fin, strides_bool)\n",
    "        start_time = fin_time[-1] + avg_int\n",
    "    return data_fin, strides_bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor():\n",
    "    # data processor; allows access of data variables later and calls readLangs()\n",
    "    # splits training and testing data too\n",
    "    def __init__(self,training_sigs=3, bin_step=100, SOS=-1, EOS = 0, one_hot = False, filter = False):\n",
    "        self.training_sigs = training_sigs\n",
    "        self.bin_step = bin_step\n",
    "        self.SOS = SOS\n",
    "        self.EOS = EOS\n",
    "        self.one_hot = one_hot\n",
    "        self.filter = filter\n",
    "\n",
    "    def input_data(self, filename):\n",
    "        self.data_all = np.genfromtxt(filename,delimiter=',')\n",
    "        self.timepts = self.data_all[:,0]\n",
    "        self.heelStrike = [np.where(self.data_all[:,7]==1)[0], np.where(self.data_all[:,-2]==1)[0]]\n",
    "        self.startPF = [np.where(self.data_all[:,7]==2)[0], np.where(self.data_all[:,-2]==2)[0]]\n",
    "        self.endPF = [np.where(self.data_all[:,7]==3)[0], np.where(self.data_all[:,-2]==3)[0]]\n",
    "        self.labels = self.data_all[:,-1]\n",
    "        self.data = self.data_all[:,[1,2,3,4,5,6,8,9,10,11,12,13]]\n",
    "        # plt.show()\n",
    "\n",
    "    def createDict(self, numbers):\n",
    "        return {j:i for i,j in enumerate(np.sort(numbers))}\n",
    "\n",
    "    def number2idx(self, numbers, my_dict):\n",
    "        return [my_dict[n] for n in numbers]\n",
    "\n",
    "    def process_data(self, two_feet=False):\n",
    "        self.data = (self.data - np.min(self.data,axis=0)/np.max(self.data,axis=0))\n",
    "        self.data = np.floor((self.data - np.min(self.data,0))/self.bin_step) + 1\n",
    "        dat.alphabet = np.unique(self.data)\n",
    "        self.alphabet_dict = self.createDict(dat.alphabet)\n",
    "        self.alphabet_data = np.array([self.number2idx(self.data[:,i], self.alphabet_dict) for i in range(self.data.shape[1])]).T\n",
    "        self.inv_alphabet_dict = {v: k for k, v in self.alphabet_dict.items()}\n",
    "        # import pdb; pdb.set_trace()\n",
    "        self.strides = [self.alphabet_data[self.endPF[0][i]:self.endPF[0][i+1],:] for i in range(len(self.endPF[0])-1)]\n",
    "        self.stridesF2 = [self.alphabet_data[self.endPF[1][i]:self.endPF[1][i+1],:] for i in range(len(self.endPF[1])-1)]\n",
    "\n",
    "        start_startPF = [np.where(self.startPF[0]>self.endPF[0][0])[0], np.where(self.startPF[1]>self.endPF[1][0])[0]]\n",
    "        # import pdb; pdb.set_trace()\n",
    "        self.localStartPF = [self.startPF[0][start_startPF[0]] - self.endPF[0][0:len(start_startPF[0])]]\n",
    "        self.localStartPF2 = [self.startPF[1][start_startPF[1]] - self.endPF[1][0:len(start_startPF[1])]]\n",
    "\n",
    "        m1 = max([len(str) for str in self.strides])\n",
    "        m2 = max([len(str) for str in self.stridesF2])\n",
    "        self.maxlength = max(m1, m2)\n",
    "        if ~two_feet:\n",
    "            self.strides.extend(self.stridesF2)\n",
    "            self.localStartPF = self.localStartPF[0].tolist()\n",
    "            self.localStartPF2 = self.localStartPF2[0].tolist()\n",
    "            self.localStartPF.extend(self.localStartPF2)\n",
    "\n",
    "    def split_tst_tr(self,tr_perc,two_feet=False):\n",
    "        num_pairs = len(self.localStartPF)\n",
    "        # import pdb; pdb.set_trace()\n",
    "        id1 = random.sample(range(num_pairs),math.floor(tr_perc*num_pairs))\n",
    "        id2 = list(set(range(num_pairs))-set(id1))\n",
    "        if ~two_feet:\n",
    "            # self.trainStrides1 = self.stridesF1[id1]\n",
    "            # self.trainStrides2 = self.stridesF2[id1]\n",
    "            # self.testStrides1 = self.stridesF1[id2]\n",
    "            # self.testStrides2 = self.stridesF2[id2]\n",
    "            #\n",
    "            # self.trainStartPF1 = self.localStartPF1[id1]\n",
    "            # self.trainStartPF2 = self.localStartPF2[id1]\n",
    "            # self.testStartPF1 = self.localStartPF1[id2]\n",
    "            # self.testStartPF2 = self.localStartPF2[id2]\n",
    "        # else:\n",
    "            self.trainStrides = np.array(self.strides)[id1].tolist()\n",
    "            self.testStrides = np.array(self.strides)[id2].tolist()\n",
    "            self.trainStartPF = np.array(self.localStartPF)[id1].tolist()\n",
    "            self.testStartPF = np.array(self.localStartPF)[id2].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    # encoder class; from https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "    def __init__(self, alphabet_size, num_features, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_features = num_features\n",
    "        self.embedding = nn.Embedding(alphabet_size, hidden_size)\n",
    "        self.input_size = alphabet_size\n",
    "        # if do_one_hot:\n",
    "        self.gru = nn.LSTM(hidden_size*self.num_features, self.input_size*self.num_features)\n",
    "    # else:\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        input = input.type(torch.long)\n",
    "        # input = input.unsqueeze(0)\n",
    "        # if self.do_one_hot:\n",
    "        embedded = torch.empty(input.shape[0], self.hidden_size)\n",
    "        for feature in range(self.num_features):\n",
    "            embedded = torch.cat((embedded, self.embedding(input[:,feature])),1)\n",
    "            # try:\n",
    "            #     embedded[feature,:,:] = self.embedding(input[:,feature])\n",
    "            # except:\n",
    "            #     import pdb; pdb.set_trace()\n",
    "        # embedded = embedded.view(-1,input.shape[0])\n",
    "        # embedded = torch.transpose(embedded,0,1)\n",
    "        embedded = embedded[:,self.hidden_size:]\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        output, hidden = self.gru(embedded)\n",
    "        output = F.softmax(output.squeeze(1),dim=1)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training function\n",
    "def train(input_tensor, startPFpt, encoder, encoder_optimizer, criterion, max_length):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    # populate hidden states with known values\n",
    "    encoder_output, encoder_hidden = encoder(input_tensor[:startPFpt-1,:], encoder_hidden)\n",
    "    input_length = input_tensor[startPFpt:,:].size(0)\n",
    "\n",
    "    input_pt = input_tensor[startPFpt,:].unsqueeze(0)\n",
    "    num_features = input_pt.shape[1]\n",
    "    try:\n",
    "        for ei in range(input_length-1):\n",
    "            encoder_output, encoder_hidden = encoder(input_pt, encoder_hidden)\n",
    "            encoder_output = encoder_output.view(1,12,-1)\n",
    "            val, input_pt = torch.max(encoder_output,2)\n",
    "            for f in range(num_features):\n",
    "                loss += criterion(encoder_output[:,f,:],input_tensor[ei+1,f].unsqueeze(0))\n",
    "    except:\n",
    "        import pdb; pdb.set_trace()\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / input_length\n",
    "\n",
    "# train for each data point; calls train\n",
    "def trainIters(encoder, epochs, train_data, trainPF, max_length,\n",
    "                print_every=1.0, plot_every=10000.0, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for iter in range(1, epochs + 1):\n",
    "        if iter == 1:\n",
    "            start = 0\n",
    "        for i in range(len(train_data)):\n",
    "            input_tensor = torch.tensor(train_data[i])\n",
    "            loss = train(input_tensor, trainPF[i],encoder,encoder_optimizer,criterion, max_length)\n",
    "            print_loss_total += loss\n",
    "            plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / epochs),\n",
    "                                         iter, iter / epochs * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "    # plt.show()\n",
    "    showPlot(plot_losses)\n",
    "\n",
    "# evaluate; same as train, but evaluates instead of training\n",
    "def evaluate(encoder, x_in, startPF, max_length):\n",
    "    with torch.no_grad():\n",
    "        input_start = x_in[:startPF,:]\n",
    "\n",
    "        real_tars = x_in[startPF:,:]\n",
    "        feature_size = input_start.shape(1)\n",
    "        input_tensor = torch.tensor(input_start[:-1,:])\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        encoder_out, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
    "        input_point = torch.tensor(input_start[-1,:]);\n",
    "        for i in range(len(real_tars)):\n",
    "            encoder_out[i,:], encoder_hidden = encoder(input_point, encoder_hidden)\n",
    "            input_point = encoder_out[i,:]\n",
    "\n",
    "        encoder_words = np.argmax(encoder_out)\n",
    "        for i in range(feature_size):\n",
    "            decoded_words[:,i] = np.array([dat.inv_alphabet_dict[encoder_outputs.item()[i,i]] for i in range(x_in.shape(0))])\n",
    "\n",
    "        return decoded_words, real_tars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-5f24c2cf06e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mencoder1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malphabet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainStrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainStartPF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# n = dat.tstx.shape[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-806ac4bbeb0f>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(encoder, epochs, train_data, trainPF, max_length, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainPF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0mprint_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mplot_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-806ac4bbeb0f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_tensor, startPFpt, encoder, encoder_optimizer, criterion, max_length)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "dat = DataProcessor()\n",
    "dat.input_data('processed_data.csv')\n",
    "dat.process_data()\n",
    "\n",
    "MAX_LENGTH = dat.maxlength\n",
    "tr_perc = .8\n",
    "dat.split_tst_tr(tr_perc)\n",
    "epochs = 1000\n",
    "#\n",
    "# xxx=[]; yyy=[];\n",
    "# start = 0\n",
    "# plt.figure()\n",
    "# for x,y in dat.tr_pairs:\n",
    "#\n",
    "#     plt.plot(range(start,len(x)+start),dat.n2idx(x, dat.in_invmap), c='g')\n",
    "#     plt.plot(range(start+len(x),len(x)+start+len(y)),dat.n2idx(y,dat.out_invmap), c = 'b')\n",
    "#     start += len(x)+len(y)+1\n",
    "#\n",
    "# plt.show()\n",
    "num_features = dat.data.shape[1]\n",
    "encoder1 = EncoderRNN(len(dat.alphabet), num_features, hidden_size).to(device)\n",
    "\n",
    "trainIters(encoder1, epochs, dat.trainStrides, dat.trainStartPF, MAX_LENGTH)\n",
    "\n",
    "# n = dat.tstx.shape[0]\n",
    "for i in range(dat.trainStrides.shape[0]):\n",
    "    x_tst = dat.trainStrides[i,:]\n",
    "    pf_start = dat.testStartPF[i]\n",
    "    predicted, attns = evaluate(encoder1, x_tst, pf_start, MAX_LENGTH)\n",
    "\n",
    "    mse_l = np.sqrt(np.sum(np.square(np.array(predicted)- np.array(y_tst))))\n",
    "    print('Testing MSE:' + str(mse_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
