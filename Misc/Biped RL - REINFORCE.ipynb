{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than the DDPG algorithm, try the REINFORCE algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HELPFUL LINKS\n",
    "My REINFORCE hw: https://github.com/jendawkins/ecehw3/blob/master/Assignment%203%20-%20Policy%20Gradient%20(DDPG%20and%20REINFORCE)%20(2).ipynb\n",
    "\n",
    "Julia Cartpole REINFORCE: https://github.com/CarloLucibello/DeepRLexamples.jl/blob/master/examples/actor_critic_pong.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brian: do you want to help with debugging this and I'll move on to the next step?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General`\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaRegistries/General.git`\n",
      "\u001b[2K\u001b[?25h[1mFetching:\u001b[22m\u001b[39m [========================================>]  100.0 %.0 %]  67.0 %\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.0/Project.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.0/Manifest.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "Pkg.add(\"Gym\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.0/Project.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.0/Manifest.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "Pkg.add(\"Knet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.0/Project.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.0/Manifest.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "Pkg.add(\"Distributions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.0/Project.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.0/Manifest.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "Pkg.add(\"Flux\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Recompiling stale cache file /Users/jenniferdawkins/.julia/compiled/v1.0/Flux/QdkVy.ji for Flux [587475ba-b771-5e3f-ad9e-33799f191a9c]\n",
      "└ @ Base loading.jl:1184\n"
     ]
    }
   ],
   "source": [
    "using Flux\n",
    "using Gym\n",
    "using Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Knet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GymEnv(\"BipedalWalker-v2\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_n = env.observation_space.shape[1]; # continuous state space\n",
    "a_n = env.action_space.shape[1]; # continuous action space\n",
    "ha_n = [25,25];\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model mlp takes in state and outputs normal distribution over states and actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Int64,1}:\n",
       " -1\n",
       " -2\n",
       " -3"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [-1,-2,-3]\n",
    "maximum(push!(x,0))\n",
    "pop!(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct History\n",
    "    states\n",
    "    actions\n",
    "    rewards\n",
    "    log_odds\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "History(Float32[], Float32[], Float32[], Float32[])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = History(zeros(Float32,0),zeros(Float32,0),zeros(Float32,0),zeros(Float32,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0-element Array{Float32,1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relu (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function relu(x::Any)\n",
    "    return maximum(push!([x],0.0))\n",
    "#     pop!(x)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "softplus (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function softplus(x)\n",
    "    return log(1+exp(x))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n"
     ]
    }
   ],
   "source": [
    "relu(3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "initweights (generic function with 3 methods)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function initweights(hidden, xsize=4, ysize=2)\n",
    "    w = []\n",
    "    x = xsize\n",
    "    for y in [hidden...]\n",
    "        push!(w, xavier(y, x))\n",
    "        push!(w, zeros(y, 1))\n",
    "        x = y\n",
    "    end\n",
    "    push!(w, xavier(ysize, x)) # Prob actions\n",
    "    push!(w, zeros(ysize, 1))\n",
    "    \n",
    "    return w\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict (generic function with 1 method)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function predict(w, x)\n",
    "    for i = 1:2:length(w)-2\n",
    "        x = relu.(w[i] * x .+ w[i+1])\n",
    "    end\n",
    "    probs = w[end-1] * x .+ w[end]\n",
    "    return probs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(Dense(24, 25, relu), Dense(25, 25, relu), BatchNorm(25), Dense(25, 2))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = Chain(\n",
    "    Dense(s_n, ha_n[1], relu),\n",
    "    Dense(ha_n[1], ha_n[2], relu),\n",
    "    BatchNorm(ha_n[2]),\n",
    "    Dense(ha_n[2], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample_action (generic function with 1 method)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function sample_action(logit,a_n)\n",
    "    dist = Normal(logit[1],softplus(logit[2]));\n",
    "    action = rand(dist,a_n);\n",
    "    log_odds = log.(pdf(dist,action));\n",
    "    return action, log_odds\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "update_policy (generic function with 2 methods)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function update_policy(w, h, gamma=1.0)\n",
    "    println(typeof(h))\n",
    "    num_paths = length(h.rewards);\n",
    "    \n",
    "    rew_cums = [];\n",
    "    lo = [0];\n",
    "    policy_loss = [];\n",
    "    p, V = predict(w, h.states)\n",
    "    for p_idx in num_paths\n",
    "        R = 0;\n",
    "        for r in reverse(history.reward[p_idx])\n",
    "            R = r + gamma*R;\n",
    "            pushfirst!(rew_cums, R)\n",
    "        end\n",
    "        append!(lo,history.log_odds[p_idx])\n",
    "    end\n",
    "    rew_cums = (rew_cums .- mean(rew_cums)) ./ (std(rew_cums) + 1e-5);\n",
    "    policy_loss = -mean(lo.*rew_cums);\n",
    "    return policy_loss\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Reinforce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-278.76446235711313\n",
      "-27.876446235711313\n",
      "Param{History}\n",
      "\n",
      "Stacktrace:\n",
      " [1] \u001b[1mgetproperty\u001b[22m\u001b[1m(\u001b[22m::Any, ::Symbol\u001b[1m)\u001b[22m at \u001b[1m./sysimg.jl:18\u001b[22m\n",
      " [2] \u001b[1mupdate_policy\u001b[22m\u001b[1m(\u001b[22m::Param{History}, ::Float64\u001b[1m)\u001b[22m at \u001b[1m./In[53]:3\u001b[22m\n",
      " [3] \u001b[1mupdate_policy\u001b[22m\u001b[1m(\u001b[22m::Param{History}\u001b[1m)\u001b[22m at \u001b[1m./In[53]:2\u001b[22m\n",
      " [4] \u001b[1m#differentiate#1\u001b[22m\u001b[1m(\u001b[22m::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::Function, ::Function, ::Param{History}\u001b[1m)\u001b[22m at \u001b[1m/Users/jenniferdawkins/.julia/packages/AutoGrad/eAmjh/src/core.jl:54\u001b[22m\n",
      " [5] \u001b[1mdifferentiate\u001b[22m\u001b[1m(\u001b[22m::Function, ::Param{History}\u001b[1m)\u001b[22m at \u001b[1m/Users/jenniferdawkins/.julia/packages/AutoGrad/eAmjh/src/core.jl:45\u001b[22m\n",
      " [6] \u001b[1m(::getfield(AutoGrad, Symbol(\"##gradfun#6#7\")){typeof(update_policy),Int64,Bool})\u001b[22m\u001b[1m(\u001b[22m::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::Function, ::History\u001b[1m)\u001b[22m at \u001b[1m/Users/jenniferdawkins/.julia/packages/AutoGrad/eAmjh/src/core.jl:139\u001b[22m\n",
      " [7] \u001b[1m(::getfield(AutoGrad, Symbol(\"#gradfun#8\")){getfield(AutoGrad, Symbol(\"##gradfun#6#7\")){typeof(update_policy),Int64,Bool}})\u001b[22m\u001b[1m(\u001b[22m::History\u001b[1m)\u001b[22m at \u001b[1m/Users/jenniferdawkins/.julia/packages/AutoGrad/eAmjh/src/core.jl:135\u001b[22m\n",
      " [8] top-level scope at \u001b[1m./In[54]:53\u001b[22m\n",
      " [9] \u001b[1meval\u001b[22m at \u001b[1m./boot.jl:319\u001b[22m [inlined]\n",
      " [10] \u001b[1msoftscope_include_string\u001b[22m\u001b[1m(\u001b[22m::Module, ::String, ::String\u001b[1m)\u001b[22m at \u001b[1m/Users/jenniferdawkins/.julia/packages/SoftGlobalScope/ujmiK/src/SoftGlobalScope.jl:206\u001b[22m\n",
      " [11] \u001b[1mexecute_request\u001b[22m\u001b[1m(\u001b[22m::ZMQ.Socket, ::IJulia.Msg\u001b[1m)\u001b[22m at \u001b[1m/Users/jenniferdawkins/.julia/packages/IJulia/0cLgR/src/execute_request.jl:224\u001b[22m\n",
      " [12] \u001b[1m#invokelatest#1\u001b[22m at \u001b[1m./essentials.jl:686\u001b[22m [inlined]\n",
      " [13] \u001b[1minvokelatest\u001b[22m at \u001b[1m./essentials.jl:685\u001b[22m [inlined]\n",
      " [14] \u001b[1meventloop\u001b[22m\u001b[1m(\u001b[22m::ZMQ.Socket\u001b[1m)\u001b[22m at \u001b[1m/Users/jenniferdawkins/.julia/packages/IJulia/0cLgR/src/eventloop.jl:8\u001b[22m\n",
      " [15] \u001b[1m(::getfield(IJulia, Symbol(\"##12#15\")))\u001b[22m\u001b[1m(\u001b[22m\u001b[1m)\u001b[22m at \u001b[1m./task.jl:259\u001b[22m"
     ]
    },
    {
     "ename": "ErrorException",
     "evalue": "type Param has no field rewards",
     "output_type": "error",
     "traceback": [
      "type Param has no field rewards",
      "",
      "Stacktrace:",
      " [1] #differentiate#1(::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::Function, ::Function, ::Param{History}) at /Users/jenniferdawkins/.julia/packages/AutoGrad/eAmjh/src/core.jl:57",
      " [2] differentiate(::Function, ::Param{History}) at /Users/jenniferdawkins/.julia/packages/AutoGrad/eAmjh/src/core.jl:45",
      " [3] (::getfield(AutoGrad, Symbol(\"##gradfun#6#7\")){typeof(update_policy),Int64,Bool})(::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::Function, ::History) at /Users/jenniferdawkins/.julia/packages/AutoGrad/eAmjh/src/core.jl:139",
      " [4] (::getfield(AutoGrad, Symbol(\"#gradfun#8\")){getfield(AutoGrad, Symbol(\"##gradfun#6#7\")){typeof(update_policy),Int64,Bool}})(::History) at /Users/jenniferdawkins/.julia/packages/AutoGrad/eAmjh/src/core.jl:135",
      " [5] top-level scope at ./In[54]:53"
     ]
    }
   ],
   "source": [
    "n_iter = 1000;\n",
    "avg_reward = 0;\n",
    "avg_rewards = [];\n",
    "step_list_rinforce = [];\n",
    "total_steps = 0;\n",
    "episodes = 0;\n",
    "lr = 1e-3\n",
    "min_timesteps_per_batch = 2000;\n",
    "w = initweights(ha_n,s_n,2);\n",
    "opt = [Adam(lr = lr) for _=1:length(w)];\n",
    "\n",
    "for itr in 1:n_iter\n",
    "    paths = [];\n",
    "    steps = 0;\n",
    "    batch_rew = 0;\n",
    "    while steps < min_timesteps_per_batch\n",
    "        ob = reset!(env)\n",
    "        obs, acs, rews, log_odds = [],[],[],[];\n",
    "        done = false;\n",
    "        ep_rew = 0;\n",
    "        while ~done\n",
    "            policy_dist = predict(w,ob)\n",
    "            action, log_odd = sample_action(policy_dist, a_n)\n",
    "            next_state, reward, done, information = step!(env, action)\n",
    "            append!(obs,ob)\n",
    "            append!(acs,action)\n",
    "            append!(rews,reward)\n",
    "            append!(log_odds,log_odd)\n",
    "            ob = next_state;\n",
    "            steps +=1;\n",
    "            ep_rew += reward\n",
    "            if done\n",
    "                episodes += 1;\n",
    "                break\n",
    "            end\n",
    "        \n",
    "        end\n",
    "        batch_rew += ep_rew;\n",
    "        append!(h.states, obs)\n",
    "        append!(h.actions,acs)\n",
    "        append!(h.rewards,rews)\n",
    "        append!(h.log_odds,log_odds)\n",
    "#         path = Dict(\"observation\"=>obs,\n",
    "#             \"reward\"=>rews,\n",
    "#             \"action\"=>acs,\n",
    "#             \"log_odds\"=>log_odds);\n",
    "#         append!(paths, path)\n",
    "    end\n",
    "    println(batch_rew)\n",
    "    avg_reward += 0.1* batch_rew + avg_reward*0.9;\n",
    "#     cumulative_reward, policy_loss = update_policy(paths);\n",
    "    println(avg_reward)\n",
    "    dw = grad(update_policy)(h)\n",
    "    update!(w,dw,opt)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(795768,)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(h.states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: `pdf(d::UnivariateDistribution, X::AbstractArray)` is deprecated, use `pdf.(d, X)` instead.\n",
      "│   caller = top-level scope at In[25]:4\n",
      "└ @ Core In[25]:4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4-element Array{Float64,1}:\n",
       " -2.4038529024248714\n",
       " -2.33690898767053  \n",
       " -3.513541775242507 \n",
       " -3.3506354417065   "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist = Normal(0,4)\n",
    "log(pdf(dist, .3);\n",
    "action = rand(dist,a_n)\n",
    "log.(pdf(dist,action))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Array{Float64,2}:\n",
       " -0.050383    0.360395   0.984606\n",
       "  0.0547567  -0.224643   0.585206\n",
       "  0.791885    0.373463  -0.857316"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand(3,3).*(2).-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0912641486376598"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode_count = 2000;\n",
    "min_epsilon = 0.01;\n",
    "decay_rate = 5/episode_count;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1 total Rewards: -85.97788289192233\n",
      "episode 2 total Rewards: -98.68037809058097\n",
      "episode 3 total Rewards: -92.41382523298962\n",
      "episode 4 total Rewards: -99.93802021494935\n",
      "episode 5 total Rewards: -79.69650269939272\n",
      "episode 6 total Rewards: -102.57114505678385\n",
      "episode 7 total Rewards: -110.65297324995726\n",
      "episode 8 total Rewards: -100.0879210012217\n",
      "episode 9 total Rewards: -98.46423048953291\n",
      "episode 10 total Rewards: -85.04734643681674\n"
     ]
    }
   ],
   "source": [
    "# General Algorithm for Q learning\n",
    "\n",
    "# For each step in each episode: \n",
    "#     Use policy defined by training network to pick actions\n",
    "#     Update experience replay\n",
    "# After each episode: \n",
    "# Sample from experience replay\n",
    "# Update training net; (update target net after x episodes)\n",
    "\n",
    "# using Gym\n",
    "reward = 0;\n",
    "episode_count = 2000;\n",
    "min_epsilon = 0.01;\n",
    "decay_rate = 5/episode_count;\n",
    "\n",
    "for i=1:episode_count\n",
    "    # set epsilon for exploring policy\n",
    "    epsilon = min_epsilon + (1.0 - min_epsilon)*exp(-decay_rate*i)\n",
    "    total = 0\n",
    "    ob = reset!(env)\n",
    "#     render(env)#comment out this line if you do not want to visualize the environment\n",
    "    while true\n",
    "        Q_values = m(ob)\n",
    "        # exploration\n",
    "        if epsilon > Random.rand()\n",
    "            action = Random.rand(env.action_space.shape[1])\n",
    "        else\n",
    "            # take max of Q_values\n",
    "#         action = sample(env.action_space)\n",
    "        end\n",
    "        ob, reward, done, information = step!(env, action)\n",
    "        total += reward\n",
    "#         render(env)#comment out this line if you do not want to visualize the environment\n",
    "        done && break\n",
    "    end\n",
    "    # Experience reply\n",
    "    # update training net \n",
    "    i%10 && \n",
    "    println(\"episode $i total Rewards: $total\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Float64,1}:\n",
       "  0.5131067509384377 \n",
       "  0.8626462959402228 \n",
       " -0.3850172680270889 \n",
       "  0.40907506858505505"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Float64,1}:\n",
       " 0.5091860357094704 \n",
       " 0.46913460209260993\n",
       " 0.03589676414598042\n",
       " 0.27638164170088775"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Random.rand(env.action_space.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State consists of hull angle speed, angular velocity, horizontal speed, vertical speed,\n",
    "# position of joints and joints angular speed, legs contact with ground, and 10 lidar\n",
    "# rangefinder measurements to help to deal with the hardcore version. There's no coordinates\n",
    "# in the state vector. Lidar is less useful in normal version, but it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoxS(Float32[-Inf, -Inf, -Inf, -Inf, -Inf, -Inf, -Inf, -Inf, -Inf, -Inf  …  -Inf, -Inf, -Inf, -Inf, -Inf, -Inf, -Inf, -Inf, -Inf, -Inf], Float32[Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf  …  Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf, Inf], (24,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoxS(Float32[-1.0, -1.0, -1.0, -1.0], Float32[1.0, 1.0, 1.0, 1.0], (4,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space\n",
    "# I think this is force on 4 joints\n",
    "# limits: -1 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "type BoxS has no field sample",
     "output_type": "error",
     "traceback": [
      "type BoxS has no field sample",
      "",
      "Stacktrace:",
      " [1] getproperty(::Any, ::Symbol) at ./sysimg.jl:18",
      " [2] top-level scope at In[44]:1"
     ]
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.0",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
